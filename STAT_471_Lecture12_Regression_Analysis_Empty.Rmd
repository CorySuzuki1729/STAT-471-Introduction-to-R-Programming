---
title: "R Notebook"
output: html_notebook
---

### Linear and Logistic Regression Analysis

As seen in STAT 410/510, the very first machine learning model that has a root in traditional statistics is the Regression model. There are many types of regression models that can be used to model relationships between multidimensional features. The most basic types of regression are linear and logistic regression models.

In elementary statistics, simple linear regression was used between two continuous numerical features as a form of bivariate quantitiative analysis. In the real world, muultiple linear regression is more useful in modeling multidimensional numerical data. The response feature is usually continuous (example: housing prices, populations in counties, weight, height, etc.) In essence, linear regression is perfect for modeling linear relationships and making predictions.

### Multiple Linear Regression

In MLR, and for many machine learning models, we want to split our data into training and testing sets. The traditional rule is the 80-20 split (80% training and 20% testing), however in real life, this split is deterministic (up to you). For example, there have been instances where if I have a small dataset, 70-30 was a better choice, and for large datasets, I've used 90-10 before. Note that the main point here is to split your data where majority of your data is part of training the model and the minority part is used for validating the accuracy and appropriateness of your model.

Here, let's consider the Boston dataset from the MASS dataset which is used in the text "Introduction to Statistical Learning with R", where we'll do a traditional 80-20 split and fit all the features. To do linear regression, we can use the lm() function and specify the regression formula with the ~ symbol. The response goes on the left of the ~ and your additive predictors go on the right.

```{r}
# install.packages("ISLR")

library(ISLR)
library(MASS)
library(rsample)

# I like doing this to check the correct split.

set.seed(42)

data_split = initial_split(Boston, prop=0.8)

train_data = training(data_split)

test_data = testing(data_split)

nrow(Boston)

nrow(train_data)

nrow(test_data)

```
Here is an example with the age and lstat features, which are being used to predict medv.

```{r}
lm.boston = lm(medv~lstat+age, data=train_data)
summary(lm.boston)
```
Now to use all features in the dataset in the formula, use a "."

```{r}
lm.boston = lm(medv~., data=train_data)
summary(lm.boston)
```

To explain the output of the model summary, this tells you the coefficient estimates generated by Least Squares Optimized linear regression, which are the coefficients to your predictor features. Note that intercept in the model summary is your y-intercept of your linear regression model. To determine how good a model is, we check many things such as the R^2 goodness of fit statistic, QQ plots, plot standardized residuals, and check the Akaike Information Criterion (AIC).

The aterisks indicate the significance levels for the p-values of each of your coefficients in the model. Traditionally 0.05 is the significance level that is used, but others such as 0.01 and 0.001 can be used. P-values can be a way to carry out feature selection, the process of improving model performance by removing unnecessary features. The hypothesis test for multiple linear regression is as follows: \

$$
H_{0}: \beta_{i} = 0 \newline
H_{a}: \beta_{i} \neq 0
$$

for some i. We reject the null hypothesis if the p-value for the coefficient is less than 0.05 and otherwise we fail to reject the null and drop the coefficient from the model. \

We will see another method of feature selection via LASSO regularized regression later in this lecture.

So in the output, it looks like age, indus, and chas are insignificant since their p-values are greater than 0.05. If we rerun the model without these features, we can expect the model's AIC to improve (smaller AIC means a better balance between features used and your goodness of fit).

```{r}
# To display the Analysis of Variance (ANOVA) table to check the sum of squares and which coefficients contribute most to the variance within the model and it's terms, you can use anova().

anova(lm.boston)
# To check the AIC of the model, use AIC().

AIC(lm.boston)

```
Let's rerun the linear regression model and see if the AIC improved. We can reruun the model by removing features with the update() function like so.

```{r}
lm.boston.new = update(lm.boston, .~.- age - chas - indus)
summary(lm.boston.new)

AIC(lm.boston.new)
```
And our AIC decreased so we now have a better model.

In regression analysis, we typically want to find the "best model". To check this in addition to AIC checks, we want to plot the residuals and standardized residuals.

```{r}
plot(lm.boston.new)
```

Comparing the QQ residual plots, it seems like the new model with the dropped terms aligns more to the normal line (dotted). This means the second model best captures the assumption of the normality of the residuals. In STAT 510, the Cook's Distance plot is used to diagnose leverage in your model, to see how data points can possibly influence your model's predictive ability. The red line that represents your model's predictive ability is straight in both so we have decent predictive power.

Thus we have done what's called "model checking and validation". Now, I will show you the non-theoretic application of what's called regularized regression.

Regularized regression comes in three flavors: LASSO, Ridge, and Elastic Net. The idea here is to start with a base model, and place penalties on terms to diagnose which ones to remove. LASSO does this in the L1 norm, Ridge does this in the L2 norm, and Elastic Net does this in the L1 and L2 norms. For simplicity, we'll only consider LASSO for this class, but I encourage you look into the other methods, which is typically covered in STAT 473: Statistical Machine Learning.

LASSO regression penalizes the sum of absolute values of your model's coefficients, which means that it places penalties on your coefficients to drive them to zero (disappearing magic trick). This is the conceptual understanding of the technique, I will not go into advanced theory here. Here's the way to do it:

```{r}
# install.packages("glmnet")

library(glmnet)
# Split the data into predictor and response matrices.

set.seed(42)

x = model.matrix(medv~., train_data)[,-1]
y = train_data$medv

# use the cross validation for glmnet to find the best lambda parameter to perform LASSO regression. use alpha=1 for LASSO and alpha=2 for Ridge.

cv_lasso = cv.glmnet(x, y, alpha=1, nfolds=10)

best_lambda = cv_lasso$lambda.min
plot(cv_lasso)
```

Now we run the final model on the data using the best alpha and display the coefficients.

```{r}
final_lm_lasso = glmnet(x,y, alpha=1, lambda=best_lambda)
coef(final_lm_lasso)
```
Now, we can see that LASSO returns the coefficients for each of our features. Here, we are confident that the age and indus coefficients are unnecessary, but what about chas? Seems like our LASSO results tell us to drop the black coefficient! That is the beauty of LASSO regression, it can give us a second opinion and confirm which features to drop from our base linear model.

Data in real life can be messy, so you may need to create different subsets of your linear models by considering nonlinear terms such as quadratic or power terms. To deal with finding optimal models raised to nonlinear powers, you can use Boxcox transformations to normalize your data and its residuals, covered in STAT 410/510 in greater detail.

To make predictions with your model, you can use the predict function on the testing data. 

```{r}
predictions.linear = predict(lm.boston.new, newdata=test_data)
predictions.linear[1:10]
```

### Logistic Regression

Now, what if our response feature isn't continuous numeric, but a binary numeric/categorical variable (whether a patient has a disease or not)? We would need to consider a special generalized linear model (GLM) called Logistic Regression. The difference here is that we would need to use the glm() function.

Another main difference between Linear Regression and Logistic Regression is that linear regression uses the identity function f(x)=x as its link function while logistic regression uses the the logistic function f(x) = ln(x/(1-x)). The link function of a generalized linear model relates the mean of the response variable to a linear combination of the predictor features. For now, let's see how we can carry out logistic regression to classify data with binary response features on the Stock Market dataset from the ISLR package.

```{r}
library(ISLR)
names(Smarket)

# The direction feature is qualitative so we drop it before using cor(). We can do this by indexing the dataset by -9 to retrieve the last 9 features.

cor(Smarket[,-9])
```

For this example, we will predict direction (either up or down hence it is binary) using the lags and stock volume features in the dataset.

```{r}
set.seed(42)

stock_split = initial_split(Smarket, prop=0.8)

train.data.stock = training(stock_split)
test.data.stock = testing(stock_split)

logistic.stock = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data = train.data.stock, family=binomial)
summary(logistic.stock)
```

Now since we have the coefficients of the logistic regression model, we can predict the probability of the levels of our response feature. In this case, we can predict whether the stock will go up or not. We can do this by using the predict() function and specifying 'type=response'.

```{r}
stock.probs = predict(logistic.stock, type="response")

# First 10 probabilities of the stock market going up

stock.probs[1:10]
```

To predict for a particular day, we convert the probabilities to class labels by specifying a binary threshold. Here, any probability greater than 0.5 will be classified as "up" and otherwise "down".

```{r}
stock.pred = rep("Down", nrow(train.data.stock))
stock.pred[stock.probs>0.5] = "Up"

# Based on the predicted probabilities, will the stock market go up or down on the 50th day, the 1000th day?

stock.pred[50]
stock.pred[1000]

```

Since logistic regression is a classification model, we can compute the accuracy of the model. We have actually calculated the accuracy of a mock classification model in the beginning of the semester with a for loop structure. Here, we can take the mean of all predictions that match with the true values from the dataset.

Alternatively, we can output what's called the "confusion matrix" of the model which gives us the classifications as True Positives, False Positives, True Negatives, and False Negatives. To compute accuracy, add up the main diagonal of the matrix and divide by the total number of rows.

```{r}
stock.acc = mean(stock.pred==train.data.stock$Direction)

# Confusion Matrix

table(stock.pred, train.data.stock$Direction)

stock.acc
stock.acc2 = (156+379) /nrow(train.data.stock)
stock.acc2
```
