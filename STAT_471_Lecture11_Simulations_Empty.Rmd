---
title: "R Notebook"
output: html_notebook
---

###**Simulations of Distributions, the CLT, and the LLN**

### Simulating Poisson Distributions

```{r}
# Generate 20 random variables from the Poisson distribution with lambda = 5, and do this for 50 random variables, and then finally for a large number

set.seed(42)

pois_data = rpois(20,5)

sim1_pois = rpois(50,5)

sim2_pois = rpois(1000, 5)

hist(pois_data, col="red", xlab="Values", ylab="frequencies", main="Histogram of 20 poisson rvs")

hist(sim1_pois, col="red", xlab="Values", ylab="frequencies", main="Histogram of 50 poisson rvs")

hist(sim2_pois, col="red", xlab="Values", ylab="frequencies", main="Histogram of 10^3 poisson rvs")

```
```{r}
plot(density(pois_data), col="blue", lwd=2, main="Density Curve for 20 Poisson RVs")

plot(density(sim1_pois), col="blue", lwd=2, main="Density Curve for 50 Poisson RVs")

plot(density(sim2_pois), col="blue", lwd=2, main="Density Curve for 10^3 Poisson RVs")

```


Both the histograms and density curves for each of the simulations are providing us a pattern.

Some useful commands:
```{r}
# Continuing with the previous simulation, let's investigate the probabilities.

# P(X<=2)

ppois(2,5)



# P(X=1)

dpois(1,5)

# P(X<x) = 0.03368973 (Finding the quantile)

qpois(0.03368973, 5)


```
Now let's check the parameters.

```{r}
mean(pois_data)
mean(sim1_pois)
mean(sim2_pois)
```
The sample means are now converging (approaching) to the population mean of 5.

<br>

### Simulating Normal Distributions

```{r}
set.seed(42)

norm_data = rnorm(20, 4, 3)

sim1_norm = rnorm(50, 4, 3)

sim2_norm = rnorm(1000, 4, 3)


# Histograms

hist(norm_data, col = "red", xlab="values", ylab="frequencies", main="Histogram of N(4,3) for 20 Simulations")

plot(density(norm_data), col="blue", lwd=2, 
     main= "PDF of N(4,3) for 20 Simulations")

hist(sim1_norm, col = "red", xlab="values", ylab="frequencies", main="Histogram of N(4,3) for 50 Simulations")

plot(density(sim1_norm), col="blue", lwd=2, 
     main= "PDF of N(4,3) for 50 Simulations")

hist(sim2_norm, col = "red", xlab="values", ylab="frequencies", main="Histogram of N(4,3) for 10^3 Simulations")

plot(density(sim2_norm), col="blue", lwd=2, 
     main= "PDF of N(4,3) for 10^3 Simulations")
```

```{r}
# Checking parameter values

mean(norm_data)
mean(sim1_norm)
mean(sim2_norm)
```
The two simulations of the Normal and Poisson distributions illustrate the magic of the CLT and LLN from a data-driven perspective. Let's take a look at some other practical examples.

You can superimpose plots like so:
```{r}
set.seed(42)
N = rnorm(1000, mean=50, sd=8)# simulate n=1000 from N(50,25)
hist(N, probability=TRUE,col="red",main="Normal distribution")
N.1 = seq(min(N), max(N), length=100)
lines(N.1, dnorm(N.1, mean=50, sd=8))

set.seed(512)
x=rnorm(100,0,1)# real observations
plot(density(x),main=" Simulated vs Real",lwd=2,xlim=c(-7,7),ylim=c(0,0.45))

x1=rnorm(100,0.5,1.5)# simulate 100 observations from N(0.5,1.5)
lines(density(x1),col="red",lwd=2)# plot x1

x2=rnorm(100,1,1)#simulate 100 observations from N(1,1)
lines(density(x2),col="blue",lwd=2)# plot x2

x3=rnorm(100,3,3)#simulate 100 observations from N(3,3)
lines(density(x3),col="green",lwd=2)# plot x3

legend(-6,0.4,c("reale","Sim 1","Sim 2","Sim 3"),fill=c("black","blue","red","green"),box.col="white")
```

<br>

### Simulating the CLT and LLN

```{r}

library(tidyverse)
# Simulate sampling from a uniform distribution
n_samples = 1000
sample_size = 50
uniform_samples = replicate(n_samples, 
                             mean(runif(sample_size, 
                                                   min = 0, max = 1)))

# Create a data frame for plotting
clt_data = data.frame(Sample_Mean = uniform_samples)

# Plot using ggplot2
ggplot(clt_data, aes(x = Sample_Mean)) +
  geom_histogram(aes(y = ..density..),
    bins = 30,
    fill = "lightblue",
    color = "black"
  ) +
  stat_function(
    fun = dnorm, args = list(
      mean = mean(uniform_samples),
      sd = sd(uniform_samples)
    ),
    color = "red", size = 1
  ) +
  labs(
    title = "Central Limit Theorem: Distribution of Sample Means",
    x = "Sample Means",
    y = "Density"
  ) +
  theme_minimal()
```

The above is the same pair of visualizations except with the ggplot package within the tidyverse package.

<br>

### Simulating Mixture Distributions: The Discrete Case

```{r}
set.seed(42)

# Component 1: Poisson distribution, lambda = 2

# Component 2: Poisson distribution, lambda = 10

lambda1 = 2

lambda2 = 10

mix_probs = c(0.7, 0.3)

n_samples = 1000

mixture_samples = numeric(n_samples)

for (i in 1:n_samples) {
  chosen_component_index = sample(1:length(mix_probs), 1, prob=mix_probs)
  if (chosen_component_index == 1) {
    mixture_samples[i] = rpois(1, lambda=lambda1)
  }
  else {
    mixture_samples[i] = rpois(1, 
lambda=lambda2)
  }
}

head(mixture_samples)

```

```{r}
hist(mixture_samples, breaks=max(mixture_samples) + 1,
     main = "Histogram of Discrete Bivariate Poisson RVs",
     xlab = "values",
     ylab = "Frequency")
```

This example provides the density histogram for a mixture distribution between two Poisson distributions with the first lambda being 2 and the second lambda being 10. Here, we mix 70% of data from the first distributional component and the remaining 30% from the second distributional component. We display the first few samples that have been randomly generated from this new mixture distribution.

<br>

### Simulating Mixture Distributions: The Continuous Case Using Transformations

```{r}
# Chi-Square

set.seed(42)

n = 1000
nu = 2
X = matrix(rnorm(n*nu), n, nu)^2
y = rowSums(X)
mean(y)

mean(y^2)

```
The above makes sense as the Chi-Squared distribution should have mean nu (2) and variance 2**nu (8) in terms of its theoretical parameters. The above approximates these well and simulate the Chi-square distribution with the sum of squared standard normal random variables.

Here is an example of a mixture distribution of two Normals.

```{r}
set.seed(42)

n = 1000

mix_probs = c(0.6, 0.4)
means = c(0, 5)
sds = c(1, 1.5)

component_labels = sample(1:2, size=n, replace=TRUE, prob=mix_probs)

mixture_samples = numeric(n)

for (i in 1:n) {
  if (component_labels[i] == 1) {
    mixture_samples[i] = rnorm(1, mean=mean[1], sd=sds[1])
  }
  else {
    mixture_samples[i] = rnorm(1, mean=mean[2], sd=sds[2])
  }
}

hist(mixture_samples, breaks=30, main = "Simulated Bivariate Normal RVs")
```

<br>

### Simulating Distributions Using the Inverse Transformation Method

The inverse CDF method can simulate distributions and their random samples by mapping samples taken from a Uniform(0,1) distribution to the input values of the original distribution's CDF. For example, suppose we have an Exponential distribution with lambda=2. The CDF for this distribution is 1-exp(-lambda*x), and its inverse is -ln(1-u) / lambda.

```{r}
set.seed(42)

n = 10000
lambda = 2

u = runif(n)

x = -log(1-u) / lambda

hist(x, probability=TRUE, col="blue", main = "Exponential via Inverse CDF")
curve(dexp(x, rate=lambda), add = TRUE, col="red", lwd=2)

head(x)
```


<br>

### Simulating Stochastic Processes

```{r}

library(tidyverse)

#White Noise

set.seed(42)

w = rnorm(500)
plot(w, type='s')
abline(h=0, col="red", lty=2, lwd=2)


#Random Walk

rw = c(0)
for (i in 2:500) {
  rw[i] = rw[i-1] + sample(c(-1,1),1)
}
plot(rw, type='s')
abline(h=0, col='red', lty=2, lwd=2)

#White Noise Moving average

w3 = stats::filter(w, filter=rep(1/3, 3), sides = 2)
plot(w3, type='s')
lines(w3, col="blue")

#White Noise Autoregressive

war = stats::filter(w, filter=c(1,-0.9), method='recursive')
plot(war, type='s')
lines(war, col="blue")

```

The above four simulations are of Gaussian white noise, a random walk, a Moving Average with Gaussian White Noise, and an Autoregressive Stochastic Process with Gaussian White Noise. These models can model basic signals (signal processing applications such as seismic detection and stock market fluctuations). We'll see some of these return when we do an introductory formal analysis of time series with real data.

<br>

### Monte Carlo Simulation Example: Approximating Pi

Monte Carlo sampling methods are the backbone for many Bayesian statistical methodologies and for today's growing complexity of Machine Learning models. Here, we'll take a look at a simple, fun example to approximate pi. We'll do this by sampling two times from the U(-0.5, 0.5) distribution and calculating the ratio of samples within a circle of radius 0.5 within a unit square divided by the total number of samples. Our approximation increases in precision as more samples are ran (think of the CLT and LLN theorems playing a role here).

```{r}
set.seed(123)

runs = 100000
xs = runif(runs, min=-0.5, max=0.5)
ys = runif(runs, min = -0.5, max=0.5)
in_circle = xs^2+ys^2 <= 0.5^2
mc_pi = (sum(in_circle)/runs)*4
plot(xs, ys, pch='.', col=ifelse(in_circle, "blue", "grey"),
     xlab = '', ylab = '', asp=1,
     main = "MC Approximation of Pi")
```


<br>
